---
title: "Supervised learning competition"
author: 
  - Aaron van Riet
  - Max Verwijmeren
  - Nelson Durañona Sosa
date: 2024-10-18
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

> Installing dependencies: 

```{r}
#| label: Install R dependencies
#| message: false

# Installing packages
if (!require(tidyverse)) {
  install.packages("tidyverse")
}

if (!require(readr)) {
  install.packages("readr")
}

if (!require(fastDummies)) {
  install.packages("fastDummies")
}

if (!require(caret)) {
  install.packages("caret")
}

if (!require(xgboost)) {
  install.packages("xgboost")
}

if (!require(corrplot)) {
  install.packages("corrplot")
}

if (!require(randomForest)) {
  install.packages("randomForest")
}
```


```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(readr)
library(ggplot2)
library(lattice)
library(class)
library(fastDummies)
library(caret)
library(xgboost)
library(corrplot)
library(knitr)
library(randomForest)
library(tidyverse)


# additional packages here
source("pre-processing_utils.R")
source("analysis_utils.R")
source("plotting_utils.R")
```

```{r}
#| label: data loading
#| echo: false

# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
```
 

# Assignment 3: Group 10, Lab 2

In this assignment, we aim to predict student performance based on various characteristics. To achieve this goal, two datasets are used: "train.rds" and "test.rds". The complete assignment can be found via the following [link](https://infomdwr.nl/assignments/assignment_3.html).

The `score` is used to measure student performance. In this assignment, several methods are tested to predict this score based on the given features. 

# Data description

The dataset consists of 316 observations and 31 variables, categorized into numerical and categorical types. Specifically, it contains **13 numerical variables** and  **18 categorical variables**. Additionally, there is no missing data in the dataset (checked with `sum(is.na(train)) == 0 `), hence was not necessary imputation methods.

For a detailed explanation of each variable and its values, please refer to the following: [Attribute overview](https://infomdwr.nl/assignments/competition_files/student.txt).

Firstly, we explored the distribution of numerical values and categorical:


```{r}
#| label: eda visualization1
#| warning: false

plots <- plot_histograms_and_bars(Train_DF)
print(plots$numeric_plot)
print(plots$categorical_plot)
```



### Correlation Matrix

To check for correlation between the variables we use a Correlation Matrix

```{r}
# Numerical columns for matrix
numerical_columns <- Train_DF[sapply(Train_DF, is.numeric)]


# Calc matrix
cor_matrix <- cor(numerical_columns, use = "complete.obs")

# Correlation matrix
corrplot(cor_matrix,
  method = "color", type = "upper",
  tl.col = "black", tl.srt = 45, addCoef.col = "black",
  number.cex = 0.7
)
```

The correlation matrix shows the relationships between the numerical variables in the dataset. Each cell displays a correlation coefficient, which ranges from -1 to 1. A coefficient close to 1 means a strong positive relationship, while a value near -1 indicates a strong negative relationship. Coefficients close to 0 suggest there's little to no linear connection between the variables.

**Score vs. Other Variables:**

-   `Score` and `Failures`: There's a moderate negative correlation (-0.41) between score and failures, meaning students with more past failures tend to score lower.

-   `Score` and `Parental Education`: The score has a positive correlation (0.27) with both `Medu` (mother's education) and `Fedu` (father's education), suggesting that students with more educated parents usually perform better.

-   `Score` and `Absences`: There’s a weak negative correlation (-0.19) between score and absences. This implies that students who miss more classes tend to score slightly lower, though the effect isn't very strong.

**Relationships Between Other Variables:**

-   `Medu` and `Fedu`: These two variables are strongly positively correlated (0.61), meaning that if a student’s mother has higher education, the father often does too.

-   `Dalc` and `Walc`: There’s a strong positive correlation (0.63) between `Dalc` (workday alcohol consumption) and `Walc` (weekend alcohol consumption), suggesting that students who drink more during the week also drink more on weekends.

-   `Failures`, `Medu`, and `Fedu`: Failures are negatively correlated with both `Medu` (-0.27) and `Fedu` (-0.28), meaning that students with more educated parents tend to have fewer past failures.


# Model description

*Briefly describe which models you compare to perform prediction. (approx. two or three paragraphs)*
For this project, and considering the size of our team, we decided to try the following models : **K-Nearest Neighbors (KNN)**, **Linear Regression**, **Decision trees-Random Forest and Decision trees- Extreme Gradient Boosting (XGBoost)**.
 
 
## 1. KNN 

  KNN is a simple model used for both classification and regression. It works by looking at the 'k' closest data points (neighbors) to make predictions based on their values. KNN doesn’t assume any specific data pattern, making it very flexible. However, it can be slow with large datasets since it has to calculate distances between points for every prediction. It’s also sensitive to how the data is scaled and the choice of 'k'.


## 2. Linear Regression

Linear Regression is one of the most basic and commonly used models for predicting continuous outcomes. It fits a straight line to the data that best explains the relationship between the input and the target variable.
We applied the following steps: 

## 3. Classification trees - Boosting  

XGBoost is a model used for both classification and regression tasks. It is an advanced version of the boosting method, where models are built sequentially, and each new model focuses on correcting the errors of the previous ones. XGBoost is known for its speed and performance, especially with large datasets, and can handle both missing data and unstructured data well. It also provides options for regularization, which helps prevent overfitting. Despite its complexity, XGBoost often delivers highly accurate predictions.

## 4. Classification trees - Random Forest 

Random Forest is a more advanced model that builds multiple decision trees and combines their predictions. It’s good for both classification and regression tasks and can handle large datasets with many features. Random Forest tends to be more accurate than single decision trees and helps reduce overfitting. However, it’s not as easy to interpret as a single decision tree, but it usually gives much better results.

# Data transformation and pre-processing

Pre-processing steps to make the data more suitable for the models. In this section, we will explain the different steps we took to prepare the data for each model. 


**Data pre-processing for KNN:**

1. Making dummy variables for categorical values
2. Apply normalization min-max 
3. Splitting data into training en test sets 80/20

```{r}
# 1. Making dummy variables for categorical values
DF_KNN <- df_categorical_to_dummyvars(Train_DF)

# 2. Apply normalization min-max
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
# Select numeric columns
numeric_columns <- sapply(DF_KNN, is.numeric)
numeric_columns["score"] <- FALSE
DF_KNN[numeric_columns] <- lapply(DF_KNN[numeric_columns], normalize)
head(DF_KNN, 2)

# 3. splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_KNN), size = 0.8 * nrow(DF_KNN))
Train_KNN_80 <- DF_KNN[indices, ]
Test_KNN_20 <- DF_KNN[-indices, ]

# splitting score from features for training set and preparing testing set
KNN_train_features <- Train_KNN_80[, -which(names(Train_KNN_80) == "score")]
KNN_train_labels <- Train_KNN_80$score

KNN_test_features <- Test_KNN_20[, -which(names(Test_KNN_20) == "score")]
KNN_test_labels <- Test_KNN_20$score
```

**Data pre-processing for Linear Regression**

1. Making dummy variables for categorical values, to obtain only numerical values.
2. Attribute selection :
We began with a simple predictor based on failures due to its strong correlation with the response variable score. Then, we performed attribute selection by:
Using cross-validation with k-folds to test each predictor.

2.1. Choosing the predictor that minimized MSE.
2.2. Repeating the process with additional predictors.
2.3. After selecting 7 predictors, we stopped as the MSE stopped improving.
The final predictors is given by: .This process was performed in the auxiliary file `linear_regression.R`.

```{r}
#| label: linear-preparation
#| warning: false


# 1. Dummy variables and split the original train set into 80% and 20%
DF_SPLITS <- Train_DF |>
  mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))

TRAIN_LN_80 <- DF_SPLITS |> filter(split == "train")
TEST_LN_20 <- DF_SPLITS |> filter(split == "test")

TRAIN_LN_80 <- df_as_numeric(TRAIN_LN_80)

# 2. Attribute selection
lm_formula <- "score~failures + sex+Medu + schoolsup + goout + romantic + Mjob_other"
```


**Data pre-processing for Random Forest**

1. Making dummy variables for categorical values since RandomForest implementation does not support categorical variables. We'll convert these variables into numeric form.
2. Splitting data into training en test sets

```{r}
# 1. Making dummy variables for categorical values
DF_RF <- df_categorical_to_dummyvars(Train_DF)

# 2. splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_RF), size = 0.8 * nrow(DF_RF))

Train_RF_80 <- DF_RF[indices, ]
Test_RF_20 <- DF_RF[-indices, ]

# splitting score from features for training set and preparing testing set
RF_train_features <- Train_RF_80[, -which(names(Train_RF_80) == "score")]
RF_train_labels <- Train_RF_80$score

RF_test_features <- Test_RF_20[, -which(names(Test_RF_20) == "score")]
RF_test_labels <- Test_RF_20$score
```

Random Forest doesn’t require feature scaling.
  
**Data pre-processing for XGBoost**

1. XGBoost does not support categorical variables. We'll convert these variables into numeric form, using dummy variables for categorical values.
2. Splitting data into training en test sets

```{r}
# 1. Making dummy variables for categorical values
DF_XGBoost <- df_categorical_to_dummyvars(Train_DF)

# 2. splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_XGBoost), size = 0.8 * nrow(DF_XGBoost))

Train_XGB_80 <- DF_XGBoost[indices, ]
Test_XGB_20 <- DF_XGBoost[-indices, ]

# splitting score from features for training set and preparing testing set
XGB_train_features <- Train_XGB_80[, -which(names(Train_XGB_80) == "score")]
XGB_train_labels <- Train_XGB_80$score

XGB_test_features <- Test_XGB_20[, -which(names(Test_XGB_20) == "score")]
XGB_test_labels <- Test_XGB_20$score
```

XGBoost doesn’t require feature scaling, unlike the first model KNN. It is tree-based and uses decision boundaries, so whether the data is standardized or not will not affect the performance.



## Model Testing

### KNN

```{r}
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
```

```{r}
knn_predictions <- as.numeric(knn_predictions)

# Calc RMSE & MSE
rmse_knn <- sqrt(mean((knn_predictions - KNN_test_labels)^2))
mse_knn <- mean((knn_predictions - KNN_test_labels)^2)

sprintf("Random Forest -->  mse = %.3f, rmse = %.3f", rmse_knn, mse_knn)
```

```{r}
# Checking Prediction vs True value of Score when with the model from the 80/20
plot_real_vs_predict(title = "KNN", True_Score = KNN_test_labels, Predicted_Score = knn_predictions)
```

###  Linear Regression

```{r}
#| label: linear-regression-1
set.seed(123)
# Training

LR_test_labels <- TEST_LN_20$score
LN_MODEL <- lm(formula = lm_formula, data = TRAIN_LN_80)
coef(LN_MODEL)


# Predicting
TEST_LN_20_num <- df_as_numeric(TEST_LN_20)
lr_predictions <- predict(LN_MODEL, newdata = TEST_LN_20_num)
```

```{r}
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - TEST_LN_20_num$score)^2)
lr_rmse <- sqrt(lr_mse)

sprintf("Linear regression -->  mse = %.3f, rmse = %.3f", lr_mse, lr_rmse)
```

```{r}
# Checking Prediction vs True value of Score when with the model from the 80/20

plot_real_vs_predict(title = "", True_Score = LR_test_labels, Predicted_Score = lr_predictions)
```

### Random Forest

```{r}
set.seed(123)
# Training
rf_model <- randomForest(x = RF_train_features, y = RF_train_labels, ntree = 100)

# Predicting
rf_predictions <- predict(rf_model, RF_test_features)
```

```{r}
# Calculate MSE and RMSE
rf_mse <- mean((rf_predictions - RF_test_labels)^2)
rf_rmse <- sqrt(rf_mse)

sprintf("Random Forest -->  mse = %.3f, rmse = %.3f", rf_mse, rf_rmse)
```

```{r}
# Checking Prediction vs True value of Score when with the model from the 80/20

plot_real_vs_predict(title = "Random Forest", True_Score = RF_test_labels, Predicted_Score = rf_predictions)
```


### Finetuning Random Forest

We tried different methods to Fine tune to model.

```{r}
set.seed(123)

# To get rid of warning: Setting row names on a tibble is deprecated
DF_RF <- as.data.frame(DF_RF)

# Using 100% of dataset
RF_features <- DF_RF[, -which(names(DF_RF) == "score")]
RF_labels <- DF_RF$score


# Defining triancontrol
control <- trainControl(method = "cv", number = 5)

# Training model
set.seed(123)
rf_cv_model <- train(
  x = RF_features,
  y = RF_labels,
  method = "rf",
  trControl = control,
  tuneLength = 5,
  ntree = 395
)


# Evaluation model
rf_predictions_cv <- predict(rf_cv_model, RF_features)
mse_cv <- mean((RF_labels - rf_predictions_cv)^2)
rmse_cv <- sqrt(mse_cv)
print(paste("MSE using cross-validation:", mse_cv))
```

```{r}
plot_real_vs_predict(title = "Random Forest fined", True_Score = RF_labels, Predicted_Score = rf_predictions_cv)
```


### XGBoost

```{r}
set.seed(123)
# Converting training and testing sets into DMatrix
XGB_dtrain <- xgb.DMatrix(data = as.matrix(XGB_train_features), label = XGB_train_labels)
XGB_dtest <- xgb.DMatrix(data = as.matrix(XGB_test_features), label = XGB_test_labels)

# hyperparameters
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Training
xgb_model <- xgb.train(params = params, data = XGB_dtrain, nrounds = 100, watchlist = list(train = XGB_dtrain, test = XGB_dtest), early_stopping_rounds = 10, print_every_n = 10)

# Evaluating
XGB_prediction <- predict(xgb_model, XGB_dtest)
```

```{r}
# Calculate MSE and RMSE
XGB_mse <- mean((XGB_prediction - XGB_test_labels)^2)
XGB_rmse <- sqrt(XGB_mse)

sprintf("XGB -->  mse = %.3f, rmse = %.3f", XGB_mse, XGB_rmse)
```

```{r}
# Checking Prediction vs True value of Score when with the model from the 80/20
plot_real_vs_predict(
  title = "XGBoost", True_Score = XGB_test_labels,
  Predicted_Score = XGB_prediction
)
```

# 5 Model Comparison

After applying four different models to predict student performance, we now compare the results using **MSE (Mean Squared Error)** and **RMSE (Root Mean Squared Error)** as evaluation metrics. These metrics give us insight into the accuracy of the models, with lower values indicating better performance.

```{r}
#| label: tbl-results
#| message: false
#|
resultsComparisson <- data.frame(
  Method = c(
    "K-Nearest Neighbors(k=5)",
    "Linear Regression",
    "DT-Random Forest",
    "DT-Random Fine-tuned",
    "DT-XGBoost"
  ),
  MSE = c(mse_knn, lr_mse, rf_mse, mse_cv, XGB_mse),
  RMSE = c(rmse_knn, lr_rmse, rf_rmse, rmse_cv, XGB_rmse)
)

head(resultsComparisson)
```
- The linear regression model had a relatively higher error compared to the other models. This is likely due to the linearity assumption, which may not always hold. It does not capture properly the complexities. 

- Random Forest builds multiple decision trees and aggregates their predictions. This model typically performs better in cases where there are complex interactions between variables.

- Random Forest showed strong performance in terms of both MSE and RMSE, with lower error than KNN and Linear Regression. This suggests that it was able to better capture the patterns in the data.

- XGBoost is an advanced ensemble learning model that uses boosting to sequentially improve weak learners. 

**XGBoost delivered the best performance in terms of both MSE and RMSE. Its boosting mechanism allowed it to effectively minimize the error over multiple rounds, making it the top-performing model in this project.**


### Storing prediction file 
 
 Finally we use the best model(DT-Random Fine-tuned) to predict the test dataset. The output is a single vector with 79 numeric values) stored in a file called `predictions.rds`:

```{r}
final_test_num <- df_categorical_to_dummyvars(Test_DF)
final_predictions <- predict(rf_cv_model, final_test_num)
write_rds(final_predictions, "predictions.rds")
summary(final_predictions)
length(final_predictions)
```




### 5.5 Conclusion

In future work, further tuning of hyperparameters for each model may improve performance even more, and exploring additional models could provide further insight into student performance prediction.



# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
