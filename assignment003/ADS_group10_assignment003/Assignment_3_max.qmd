---
title: "Assignment 3, Lab 2, Group 10"
format: html
editor: visual
---

# Assignment 3: Group 10, Lab 2

In this assignment, we aim to predict student performance based on various characteristics. To achieve this goal, two datasets are used: "train.rds" and "test.rds". The complete assignment can be found via the following [link](https://infomdwr.nl/assignments/assignment_3.html).

Score is used to measure student performance. In this assignment, several methods are tested to predict this score based on the given features. The model that performed best in the tests was then fine-tuned to optimize the prediction of the score.

## 1. Data Description & Data Exploration

In this part, we will examine the structure and content of the dataset to gain an understanding of the available information.

```{r}
#Installing packages
if (!require(tidyverse)) {
  install.packages("tidyverse")
}

if (!require(readr)) {
  install.packages("readr")
}

if (!require(fastDummies)) {
  install.packages("fastDummies")
}

if (!require(caret)) {
  install.packages("caret")
}

if (!require(xgboost)) {
  install.packages("xgboost")
}

if (!require(corrplot)) {
  install.packages("corrplot")
}

if (!require(randomForest)) {
  install.packages("randomForest")
}
if (!require(caretEnsemble)) {
  install.packages("caretEnsemble")
}

if (!require(ranger)) {
  install.packages("ranger")
}
```

```{r}
#Importing libraries
library(readr)
library(ggplot2)
library(lattice)
library(class)
library(fastDummies) 
library(caret)
library(xgboost)
library(corrplot)
library(randomForest)
library(caretEnsemble)
library(ranger)
```

```{r}
url <- "https://github.com/MaxVer01/DataWrangling_Lab2_Group10/raw/main/train.rds"
temp_file <- tempfile(fileext = ".rds")
download.file(url, destfile = temp_file, mode = "wb")
```

```{r}
#Reading data
Train_DF <- readRDS(temp_file)
```

```{r}
#Checking structure
str(Train_DF)

#Checking for missing values
sum(is.na(Train_DF))
```

```{r}
set.seed(126)
```

### 1.2 Structure and Missing Values

The dataset consists of 316 observations and 31 variables, categorized into numerical and categorical types. Specifically, it contains:

-   **13 numerical variables**,

-   **18 categorical variables**,

Additionally, there is no missing data in the dataset.

For a detailed explanation of each variable and its values, please refer to the following: [Attribute overview](https://infomdwr.nl/assignments/competition_files/student.txt).

### **1.3 Overview of Numerical and Categorical Variables**

```{r}
#For numerical data we use histograms
numerical_columns <- names(Train_DF)[sapply(Train_DF, is.numeric)]

# Loop
for (col_name in numerical_columns) {
  print(
    ggplot(Train_DF, aes_string(x = col_name)) + 
      geom_histogram(binwidth = 1, fill = "blue", color = "black") +
      ggtitle(paste("Histogram of", col_name)) +
      xlab(col_name) +
      ylab("Frequency") +
      theme_minimal()
  )
}
```

```{r}
#For categorical variables we use Bar Charts
categorical_columns <- names(Train_DF)[sapply(Train_DF, is.factor)]

#loop
for (col_name in categorical_columns) {
  print(
    ggplot(Train_DF, aes_string(x = col_name)) + 
      geom_bar(fill = "blue", color = "black") +
      ggtitle(paste("Bar chart of", col_name)) +
      xlab(col_name) +
      ylab("Count") +
      theme_minimal()
  )
}
```

#### 1.3.1 Outliers

Based on the figures and the information provided for the variables, we can conclude that there are no outliers in the dataset.

### 1.4 Correlation Matrix

To check for correlation between the variables we use a Correlation Matrix

```{r}
#Numerical columns for matrix
numerical_columns <- Train_DF[sapply(Train_DF, is.numeric)]


#Calc matrix
cor_matrix <- cor(numerical_columns, use = "complete.obs")

# Correlation matrix
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black", 
         number.cex = 0.7)
```

The correlation matrix shows the relationships between the numerical variables in the dataset. Each cell displays a correlation coefficient, which ranges from -1 to 1. A coefficient close to 1 means a strong positive relationship, while a value near -1 indicates a strong negative relationship. Coefficients close to 0 suggest there's little to no linear connection between the variables.

#### 1.4.1 Observations from the Matrix:

**Score vs. Other Variables:**

-   **Score and Failures**: There's a moderate negative correlation (-0.41) between score and failures, meaning students with more past failures tend to score lower.

-   **Score and Parental Education**: The score has a positive correlation (0.27) with both **Medu** (mother's education) and **Fedu** (father's education), suggesting that students with more educated parents usually perform better.

-   **Score and Absences**: There’s a weak negative correlation (-0.19) between score and absences. This implies that students who miss more classes tend to score slightly lower, though the effect isn't very strong.

**Relationships Between Other Variables:**

-   **Medu and Fedu**: These two variables are strongly positively correlated (0.61), meaning that if a student’s mother has higher education, the father often does too.

-   **Dalc and Walc**: There’s a strong positive correlation (0.63) between **Dalc** (workday alcohol consumption) and **Walc** (weekend alcohol consumption), suggesting that students who drink more during the week also drink more on weekends.

-   **Failures, Medu, and Fedu**: Failures are negatively correlated with both **Medu** (-0.27) and **Fedu** (-0.28), meaning that students with more educated parents tend to have fewer past failures.

## **2. Models Description**

To create an effective supervised learning model, it’s important to fully understand the data, which is why we started with Exploratory Data Analysis (EDA). While EDA gives us valuable insights, the choice of model is also key to achieving good results. For this project, and considering the size of our group, we decided to try three models: **K-Nearest Neighbors (KNN)**, **Linear Regression**, **Random Forest and Extreme Gradient Boosting (XGBoost)**.\
\
**Explanation of each model:**

-   **KNN**

    KNN is a simple model used for both classification and regression. It works by looking at the 'k' closest data points (neighbors) to make predictions based on their values. KNN doesn’t assume any specific data pattern, making it very flexible. However, it can be slow with large datasets since it has to calculate distances between points for every prediction. It’s also sensitive to how the data is scaled and the choice of 'k'.

-   **Linear Regression**

    Linear Regression is one of the most basic and commonly used models for predicting continuous outcomes. It fits a straight line to the data that best explains the relationship between the input and the target variable. This model is easy to understand and quick to run. However, it assumes a straight-line relationship, so it may not work well with more complex patterns in the data.

-   **Random Forest**

    Random Forest is a more advanced model that builds multiple decision trees and combines their predictions. It’s good for both classification and regression tasks and can handle large datasets with many features. Random Forest tends to be more accurate than single decision trees and helps reduce overfitting. However, it’s not as easy to interpret as a single decision tree, but it usually gives much better results.

-   **XGBoost**

    XGBoost is a model used for both classification and regression tasks. It is an advanced version of the boosting method, where models are built sequentially, and each new model focuses on correcting the errors of the previous ones. XGBoost is known for its speed and performance, especially with large datasets, and can handle both missing data and unstructured data well. It also provides options for regularization, which helps prevent overfitting. Despite its complexity, XGBoost often delivers highly accurate predictions.

## 3. Data Preprocessing

Preparing the data is an important part of the data processing pipeline. The models we described won’t work well if they are trained on the raw data from the dataframe. That's why it's important to take some preprocessing steps to make the data more suitable for the models. In this section, we will explain the different steps we took to prepare the data for each model.

Because of the different models, and the chance that the steps mess up the other models. We will assign each model with its own df.

### 3.1 Data Preprocessing for KNN

```{r}
#Setting up dataframe
DF_KNN = Train_DF
```

-   **Step 1: Making dummy variables for categorical values**

```{r}
DF_KNN <- dummy_cols(DF_KNN, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)

# Removing original categorical columns
DF_KNN <- DF_KNN[, !names(DF_KNN) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup", 
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
```

-   **Step 2: Normalizing numerical values**

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

#Select numeric columns
numeric_columns <- sapply(DF_KNN, is.numeric)
numeric_columns["score"] <- FALSE
DF_KNN[numeric_columns] <- lapply(DF_KNN[numeric_columns], normalize)

#Checking DF
head(DF_KNN)
```

-   **Step 3a: Splitting data into training en test sets 80/20**

    ```{r}
    #splitting the train dataset to make a test dataframe (80%-20%)
    indices <- sample(1:nrow(DF_KNN), size = 0.8 * nrow(DF_KNN))

    Train_KNN_80 <- DF_KNN[indices, ]
    Test_KNN_20 <- DF_KNN[-indices, ]

    # splitting score from features for training set and preparing testing set
    KNN_train_features <- Train_KNN_80[, -which(names(Train_KNN_80) == "score")]
    KNN_train_labels <- Train_KNN_80$score

    KNN_test_features <- Test_KNN_20[, -which(names(Test_KNN_20) == "score")]
    KNN_test_labels <- Test_KNN_20$score
    ```

-   **Step 3b: Crossvalidation**

```{r}
DF_KNN <- as.data.frame(DF_KNN)

#Using crossvalidation on the data instead of 80/20 training set
#Cross_validation method
Train_control_KNN <- trainControl(method = "cv", number = 10)

# Splitting
KNN_features <- DF_KNN[, -which(names(DF_KNN) == "score")]
KNN_labels <- DF_KNN$score
```

### 3.2 Data Preprocessing for Linear Regression

```{r}
DF_LR = Train_DF
```

-   **Step 1: Making dummy variables for categorical values**

    Linear Regression does not support categorical variables. We'll convert these variables into numeric form.

```{r}
DF_LR <- dummy_cols(DF_LR, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)

# Removing original categorical columns
DF_LR <- DF_LR[, !names(DF_LR) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup", 
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
```

-   **Step 2: Normalizing numerical values**

    ```{r}
    normalize <- function(x) {
      return ((x - min(x)) / (max(x) - min(x)))
    }

    #Select numeric columns
    numeric_columns <- sapply(DF_LR, is.numeric)
    numeric_columns["score"] <- FALSE
    DF_LR[numeric_columns] <- lapply(DF_LR[numeric_columns], normalize)

    #Checking DF
    head(DF_LR)
    ```

-   **Step 3:** **Splitting data into training en test sets**

    ```{r}
    #splitting the train dataset to make a test dataframe (80%-20%)
    indices <- sample(1:nrow(DF_LR), size = 0.8 * nrow(DF_LR))

    Train_LR_80 <- DF_LR[indices, ]
    Test_LR_20 <- DF_LR[-indices, ]

    # splitting score from features for training set and preparing testing set
    LR_train_features <- Train_LR_80[, -which(names(Train_LR_80) == "score")]
    LR_train_labels <- Train_LR_80$score

    LR_test_features <- Test_LR_20[, -which(names(Test_LR_20) == "score")]
    LR_test_labels <- Test_LR_20$score
    ```

### 3.3 Data Preprocessing for Random Forest

```{r}
DF_RF = Train_DF
```

-   **Step 1: Making dummy variables for categorical values**

    RandomForest does not support categorical variables. We'll convert these variables into numeric form.

```{r}
DF_RF <- dummy_cols(DF_RF, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)

# Removing original categorical columns
DF_RF <- DF_RF[, !names(DF_RF) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup", 
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
```

Random Forest doesn’t require feature scaling.

-   **Step 2: Splitting data into training en test sets**

    ```{r}
    #splitting the train dataset to make a test dataframe (80%-20%)
    indices <- sample(1:nrow(DF_RF), size = 0.8 * nrow(DF_RF))

    Train_RF_80 <- DF_RF[indices, ]
    Test_RF_20 <- DF_RF[-indices, ]

    # splitting score from features for training set and preparing testing set
    RF_train_features <- Train_RF_80[, -which(names(Train_RF_80) == "score")]
    RF_train_labels <- Train_RF_80$score

    RF_test_features <- Test_RF_20[, -which(names(Test_RF_20) == "score")]
    RF_test_labels <- Test_RF_20$score
    ```

### 3.4 Data Preprocessing for XGBoost

```{r}
DF_XGBoost = Train_DF
```

-   **Step 1: Making dummy variables for categorical values**

XGBoost does not support categorical variables. We'll convert these variables into numeric form.

```{r}
DF_XGBoost <- dummy_cols(DF_XGBoost, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)

# Removing original categorical columns
DF_XGBoost <- DF_XGBoost[, !names(DF_XGBoost) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup", 
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
```

XGBoost doesn’t require feature scaling, unlike the first model KNN. It is tree-based and uses decision boundaries, so whether the data is standardized or not will not affect the performance.

-   **Step 2: Splitting data into training en test sets**

    ```{r}
    #splitting the train dataset to make a test dataframe (80%-20%)
    indices <- sample(1:nrow(DF_XGBoost), size = 0.8 * nrow(DF_XGBoost))

    Train_XGB_80 <- DF_XGBoost[indices, ]
    Test_XGB_20 <- DF_XGBoost[-indices, ]

    # splitting score from features for training set and preparing testing set
    XGB_train_features <- Train_XGB_80[, -which(names(Train_XGB_80) == "score")]
    XGB_train_labels <- Train_XGB_80$score

    XGB_test_features <- Test_XGB_20[, -which(names(Test_XGB_20) == "score")]
    XGB_test_labels <- Test_XGB_20$score
    ```

## 4 Model Testing

### 4.1a KNN 80/20

```{r}
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
```

```{r}
knn_predictions <- as.numeric(knn_predictions)

# Calc RMSE & MSE
rmse_knn <- sqrt(mean((knn_predictions - KNN_test_labels)^2))
mse_knn <- mean((knn_predictions - KNN_test_labels)^2)

rmse_knn
mse_knn

```

```{r}
#Checking Prediction vs True value of Score when with the model from the 80/20
results_knn <- data.frame(True_Score = KNN_test_labels, Predicted_Score = knn_predictions)

ggplot(results_knn, aes(x = True_Score, y = Predicted_Score)) +
  geom_point(color = "blue", alpha = 0.6) + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "KNN: Prediction vs True value of Score",
       x = "True Score",
       y = "Predicted Score") +
  theme_minimal()
```

### 4.2 Linear Regression

```{r}
set.seed(123)
# Training
lm_model <- lm(score ~ ., data = Train_LR_80)

# Predicting
lr_predictions <- predict(lm_model, newdata = Test_LR_20)
```

```{r}
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - LR_test_labels) ^ 2)   
lr_rmse <- sqrt(lr_mse)                                

lr_mse
lr_rmse
```

```{r}
#Checking Prediction vs True value of Score when with the model from the 80/20
results_lr <- data.frame(True_Score = LR_test_labels, Predicted_Score = lr_predictions)


ggplot(results_lr, aes(x = True_Score, y = Predicted_Score)) +
  geom_point(color = "blue", alpha = 0.6) + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Linear Regression: Prediction vs True value of Score",
       x = "True Score",
       y = "Predicted Score") +
  theme_minimal()
```

### 4.3 Random Forest

```{r}
set.seed(123)
# Training
rf_model <- randomForest(x = RF_train_features, y = RF_train_labels, ntree = 100)

# Predicting
rf_predictions <- predict(rf_model, RF_test_features)
```

```{r}
# Calculate MSE and RMSE
rf_mse <- mean((rf_predictions - RF_test_labels) ^ 2)   
rf_rmse <- sqrt(rf_mse)                                 

rf_mse
rf_rmse
```

```{r}
#Checking Prediction vs True value of Score when with the model from the 80/20
results_rf <- data.frame(True_Score = RF_test_labels, Predicted_Score = rf_predictions)

ggplot(results_rf, aes(x = True_Score, y = Predicted_Score)) +
  geom_point(color = "blue", alpha = 0.6) + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Random Forest: Prediction vs True value of Score",
       x = "True Score",
       y = "Predicted Score") +
  theme_minimal()
```

### 4.4 XGBoost

```{r}
set.seed(123)
# Converting training and testing sets into DMatrix
XGB_dtrain <- xgb.DMatrix(data = as.matrix(XGB_train_features), label = XGB_train_labels)
XGB_dtest <- xgb.DMatrix(data = as.matrix(XGB_test_features), label = XGB_test_labels)

# hyperparameters 
params <- list(objective = "reg:squarederror", 
               eval_metric = "rmse",            
               eta = 0.1,                       
               max_depth = 6,                   
               subsample = 0.8,                 
               colsample_bytree = 0.8)

# Training
xgb_model <- xgb.train(params = params,data = XGB_dtrain,nrounds = 100, watchlist = list(train = XGB_dtrain, test = XGB_dtest),early_stopping_rounds = 10, print_every_n = 10)

# Evaluating
XGB_prediction <- predict(xgb_model, XGB_dtest)
```

```{r}
# Calculate MSE and RMSE
XGB_mse <- mean((XGB_prediction - XGB_test_labels) ^ 2)   
XGB_rmse <- sqrt(XGB_mse)                            

XGB_mse
XGB_rmse

```

```{r}
#Checking Prediction vs True value of Score when with the model from the 80/20
results_xgb <- data.frame(True_Score = XGB_test_labels, Predicted_Score = XGB_prediction)

ggplot(results_xgb, aes(x = True_Score, y = Predicted_Score)) +
  geom_point(color = "blue", alpha = 0.6) + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "XGBoost: Prediction vs True value of Score",
       x = "True Score",
       y = "Predicted Score") +
  theme_minimal()
```

### 5 Model Comparison

After applying four different models to predict student performance, we now compare the results using **MSE (Mean Squared Error)** and **RMSE (Root Mean Squared Error)** as evaluation metrics. These metrics give us insight into the accuracy of the models, with lower values indicating better performance.

#### 5.1 K-Nearest Neighbors (KNN)

KNN was trained with k=5, and the results show how the model performed in predicting the scores.

-   **MSE**:

    ```{r}
    mse_knn
    ```

-   **RMSE**:

```{r}
rmse_knn
```

#### 5.2 Linear Regression

-   **MSE**:

    ```{r}
    lr_mse
    ```

-   **RMSE**:

```{r}
lr_rmse
```

#### 5.3 Random Forest

-   **MSE**:

    ```{r}
    rf_mse
    ```

-   **RMSE**:

    ```{r}
    rf_mse
    ```

#### 5.4 XGBoost

-   **MSE**:

    ```{r}
    XGB_mse
    ```

-   **RMSE**:

    ```{r}
    XGB_rmse
    ```

Random Forest delivered the best performance in terms of both MSE and RMSE.

### **5.5 Conclusion**

Random Forest delivered the best performance in terms of both MSE and RMSE results.

## 6. Finetuning Random Forest
