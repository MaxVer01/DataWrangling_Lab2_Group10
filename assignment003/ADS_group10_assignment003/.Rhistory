plot_histograms_and_bars <- function(data) {
numeric_cols <- data %>% select_if(is.numeric) %>% gather(key = "column", value = "value")
categorical_cols <- data %>% select_if(is.factor) %>% gather(key = "column", value = "value")
# Plot histograms for numeric columns
numeric_plot <- ggplot(numeric_cols, aes(x = value)) +
geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Histograms of Numeric Columns", x = "Values", y = "Frequency") +
theme_minimal()
# Plot bar plots for categorical columns
categorical_plot <- ggplot(categorical_cols, aes(x = value)) +
geom_bar(fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Bar Plots of Categorical Columns", x = "Categories", y = "Count") +
theme_minimal()
# Return the two plots
list(numeric_plot = numeric_plot, categorical_plot = categorical_plot)
}
plots <- plot_histograms_and_bars(train)
print(plots$numeric_plot)
print(plots$categorical_plot)
categorical_cols <- train %>% select_if(is.factor) %>% gather(key = "column", value = "value", -score)
categorical_cols <- train %>%  select(where(~ is.factor(.) || colnames(data) == "score")) %>% gather(key = "column", value = "value", -score)
categorical_cols <- train %>%  select(where(~ is.factor(.) || colnames(train) == "score")) %>% gather(key = "column", value = "value", -score)
plot_histograms_and_bars <- function(data) {
numeric_cols <- data %>% select_if(is.numeric) %>% gather(key = "column", value = "value")
categorical_cols <- data %>% select_if(is.factor) %>% gather(key = "column", value = "value")
# Plot histograms for numeric columns
numeric_plot <- ggplot(numeric_cols, aes(x = value)) +
geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Histograms of Numeric Columns", x = "Values", y = "Frequency") +
theme_minimal()
# Plot bar plots for categorical columns
categorical_plot <- ggplot(categorical_cols, aes(x = value)) +
geom_bar(fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Bar Plots of Categorical Columns", x = "Categories", y = "Count") +
theme_minimal()
# Return the two plots
list(numeric_plot = numeric_plot, categorical_plot = categorical_plot)
}
plot_histograms_and_bars <- function(data) {
numeric_cols <- data %>% select_if(is.numeric) %>% gather(key = "column", value = "value")
categorical_cols <- data %>% select_if(is.factor) %>% gather(key = "column", value = "value")
# Plot histograms for numeric columns
numeric_plot <- ggplot(numeric_cols, aes(x = value)) +
geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Histograms of Numeric Columns", x = "Values", y = "Frequency") +
theme_minimal()
# Plot bar plots for categorical columns
categorical_plot <- ggplot(categorical_cols, aes(x = value)) +
geom_bar(fill = "blue", color = "black", alpha = 0.7) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Bar Plots of Categorical Columns", x = "Categories", y = "Count") +
theme_minimal()
# Return the two plots
list(numeric_plot = numeric_plot, categorical_plot = categorical_plot)
}
plots <- plot_histograms_and_bars(train)
print(plots$numeric_plot)
plots <- plot_histograms_and_bars(train)
print(plots$numeric_plot)
print(plots$categorical_plot)
plot_histograms_and_bars <- function(data) {
numeric_cols <- data %>% select_if(is.numeric) %>% gather(key = "column", value = "value")
categorical_cols <- data %>% select_if(is.factor) %>% gather(key = "column", value = "value")
# Plot histograms for numeric columns
numeric_plot <- ggplot(numeric_cols, aes(x = value)) +
geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.6) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Histograms of numeric columns", x = "Values", y = "Freq") +
theme_minimal()
# Plot bar plots for categorical columns
categorical_plot <- ggplot(categorical_cols, aes(x = value)) +
geom_bar(fill = "green", color = "black", alpha = 0.6) +
facet_wrap(~column, scales = "free_x") +
labs(title = "Bar Plots of categorical columns", x = "categories", y = "Count") +
theme_minimal()
# Return the two plots
list(numeric_plot = numeric_plot, categorical_plot = categorical_plot)
}
plots <- plot_histograms_and_bars(train)
print(plots$numeric_plot)
print(plots$categorical_plot)
sum(is.na(train))
sum(is.na(train)) == 0
source("~/.active-rstudio-document", echo=TRUE)
summary(train)
View(corrdf)
View(corrdf)
View(corrdf)
View(corrdf)
View(train)
View(train)
linearReg.model <- lm(formula = linearReg.formula, data = train.numeric)
coef(linearReg.model)
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
#| label: R packages
#| echo: false
#| warning: false
#| message: false
library(readr)
library(ggplot2)
library(lattice)
library(class)
library(fastDummies)
library(caret)
library(xgboost)
library(corrplot)
library(randomForest)
library(tidyverse)
# additional packages here
source("pre-processing_utils.R")
source("analysis_utils.R")
source("plotting_utils.R")
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset <- train.splits |> filter(split == "train")
test.unseen <- train.splits |> filter(split == "test")
train.numeric <- df_as_numeric(train)
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset <- train.splits |> filter(split == "train")
test.unseen <- train.splits |> filter(split == "test")
train.numeric <- df_as_numeric(Train_DF)
#| label: eda visualization1
#| warning: false
plots <- plot_histograms_and_bars(Train_DF)
print(plots$numeric_plot)
print(plots$categorical_plot)
#Setting up dataframe
DF_KNN <- Train_DF
DF_KNN <- dummy_cols(DF_KNN, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)
# Removing original categorical columns
DF_KNN <- DF_KNN[, !names(DF_KNN) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup",
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#Select numeric columns
numeric_columns <- sapply(DF_KNN, is.numeric)
numeric_columns["score"] <- FALSE
DF_KNN[numeric_columns] <- lapply(DF_KNN[numeric_columns], normalize)
#Checking DF
head(DF_KNN)
#splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_KNN), size = 0.8 * nrow(DF_KNN))
Train_KNN_80 <- DF_KNN[indices, ]
Test_KNN_20 <- DF_KNN[-indices, ]
# splitting score from features for training set and preparing testing set
KNN_train_features <- Train_KNN_80[, -which(names(Train_KNN_80) == "score")]
KNN_train_labels <- Train_KNN_80$score
KNN_test_features <- Test_KNN_20[, -which(names(Test_KNN_20) == "score")]
KNN_test_labels <- Test_KNN_20$score
set.seed(123)
# Training
lm_model <- lm(lm_formula, data = df_as_numeric(train.subset80))
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset80 <- train.splits |> filter(split == "train")
test.unseen20 <- train.splits |> filter(split == "test")
train.numeric <- df_as_numeric(train.subset80F)
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset80 <- train.splits |> filter(split == "train")
test.unseen20 <- train.splits |> filter(split == "test")
train.numeric <- df_as_numeric(train.subset80F)
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset80 <- train.splits |> filter(split == "train")
test.unseen20 <- train.splits |> filter(split == "test")
#| label: eda visualization2
#| warning: false
lm_formula <- 'score~failures + sex+Medu + schoolsup + goout + romantic + Mjob_other'
train.numeric <- df_as_numeric(train.subset80F)
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset80 <- train.splits |> filter(split == "train")
test.unseen20 <- train.splits |> filter(split == "test")
#| label: eda visualization2
#| warning: false
lm_formula <- 'score~failures + sex+Medu + schoolsup + goout + romantic + Mjob_other'
train.numeric <- df_as_numeric(train.subset80)
linearReg.model <- lm(formula = lm_formula, data = train.numeric)
coef(linearReg.model)
#| label: eda visualization2
#| warning: false
lm_formula <- 'score~failures + sex+Medu + schoolsup + goout + romantic + Mjob_other'
train.numeric <- df_as_numeric(train.subset80)
linearReg.model <- lm(formula = lm_formula, data = train.numeric)
coef(linearReg.model)
set.seed(123)
# Training
# Predicting
lr_predictions <- predict(linearReg.model, newdata = df_as_numeric(test.unseen20) )
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - LR_test_labels) ^ 2)
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - unseen.num$score) ^ 2)
set.seed(123)
# Training
# Predicting
unseen.num <- df_as_numeric(test.unseen20)
lr_predictions <- predict(linearReg.model, newdata =unseen.num  )
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - unseen.num$score) ^ 2)
lr_rmse <- sqrt(lr_mse)
lr_mse
lr_rmse
set.seed(123)
# Training
LR_test_labels <- test.unseen20$score
# Predicting
unseen.num <- df_as_numeric(test.unseen20)
lr_predictions <- predict(linearReg.model, newdata =unseen.num  )
View(Train_KNN_80)
View(Train_KNN_80)
View(KNN_train_features)
View(KNN_train_features)
knn_predictions <- as.numeric(knn_predictions)
knn_predictions <- as.numeric(knn_predictions)
knn_predictions <- as.numeric(knn_predictions)
#| label: Install R dependencies
#| message: false
#Installing packages
if (!require(tidyverse)) {
install.packages("tidyverse")
}
if (!require(readr)) {
install.packages("readr")
}
if (!require(fastDummies)) {
install.packages("fastDummies")
}
if (!require(caret)) {
install.packages("caret")
}
if (!require(xgboost)) {
install.packages("xgboost")
}
if (!require(corrplot)) {
install.packages("corrplot")
}
if (!require(randomForest)) {
install.packages("randomForest")
}
#| label: R packages
#| echo: false
#| warning: false
#| message: false
library(readr)
library(ggplot2)
library(lattice)
library(class)
library(fastDummies)
library(caret)
library(xgboost)
library(corrplot)
library(randomForest)
library(tidyverse)
# additional packages here
source("pre-processing_utils.R")
source("analysis_utils.R")
source("plotting_utils.R")
#| label: data loading
#| echo: false
# your R code to load the data here
Train_DF <- readRDS("raw_data/train.rds")
Test_DF <- readRDS("raw_data/test.rds")
### Split the original train set into 80% and 20%
train.splits <- Train_DF |>
mutate(split = sample(rep(c("train", "test"), times = c(253, 63))))
train.subset80 <- train.splits |> filter(split == "train")
test.unseen20 <- train.splits |> filter(split == "test")
#| label: eda visualization1
#| warning: false
plots <- plot_histograms_and_bars(Train_DF)
print(plots$numeric_plot)
print(plots$categorical_plot)
#Numerical columns for matrix
numerical_columns <- Train_DF[sapply(Train_DF, is.numeric)]
#Calc matrix
cor_matrix <- cor(numerical_columns, use = "complete.obs")
# Correlation matrix
corrplot(cor_matrix, method = "color", type = "upper",
tl.col = "black", tl.srt = 45, addCoef.col = "black",
number.cex = 0.7)
#Setting up dataframe
DF_KNN <- Train_DF
DF_KNN <- dummy_cols(DF_KNN, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)
# Removing original categorical columns
DF_KNN <- DF_KNN[, !names(DF_KNN) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup",
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#Select numeric columns
numeric_columns <- sapply(DF_KNN, is.numeric)
numeric_columns["score"] <- FALSE
DF_KNN[numeric_columns] <- lapply(DF_KNN[numeric_columns], normalize)
#Checking DF
head(DF_KNN)
#splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_KNN), size = 0.8 * nrow(DF_KNN))
Train_KNN_80 <- DF_KNN[indices, ]
Test_KNN_20 <- DF_KNN[-indices, ]
# splitting score from features for training set and preparing testing set
KNN_train_features <- Train_KNN_80[, -which(names(Train_KNN_80) == "score")]
KNN_train_labels <- Train_KNN_80$score
KNN_test_features <- Test_KNN_20[, -which(names(Test_KNN_20) == "score")]
KNN_test_labels <- Test_KNN_20$score
#| label: eda visualization2
#| warning: false
lm_formula <- 'score~failures + sex+Medu + schoolsup + goout + romantic + Mjob_other'
train.numeric <- df_as_numeric(train.subset80)
linearReg.model <- lm(formula = lm_formula, data = train.numeric)
coef(linearReg.model)
DF_RF = Train_DF
DF_RF <- dummy_cols(DF_RF, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)
# Removing original categorical columns
DF_RF <- DF_RF[, !names(DF_RF) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup",
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
#splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_RF), size = 0.8 * nrow(DF_RF))
Train_RF_80 <- DF_RF[indices, ]
Test_RF_20 <- DF_RF[-indices, ]
# splitting score from features for training set and preparing testing set
RF_train_features <- Train_RF_80[, -which(names(Train_RF_80) == "score")]
RF_train_labels <- Train_RF_80$score
RF_test_features <- Test_RF_20[, -which(names(Test_RF_20) == "score")]
RF_test_labels <- Test_RF_20$score
DF_XGBoost = Train_DF
DF_XGBoost <- dummy_cols(DF_XGBoost, select_columns = c("school", "sex", "address","famsize", "Pstatus","Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic"), remove_first_dummy = TRUE)
# Removing original categorical columns
DF_XGBoost <- DF_XGBoost[, !names(DF_XGBoost) %in% c("school", "sex", "address", "famsize", "Pstatus",  "Mjob", "Fjob", "reason", "guardian", "schoolsup",
"famsup", "paid", "activities", "nursery", "higher","internet", "romantic")]
#splitting the train dataset to make a test dataframe (80%-20%)
indices <- sample(1:nrow(DF_XGBoost), size = 0.8 * nrow(DF_XGBoost))
Train_XGB_80 <- DF_XGBoost[indices, ]
Test_XGB_20 <- DF_XGBoost[-indices, ]
# splitting score from features for training set and preparing testing set
XGB_train_features <- Train_XGB_80[, -which(names(Train_XGB_80) == "score")]
XGB_train_labels <- Train_XGB_80$score
XGB_test_features <- Test_XGB_20[, -which(names(Test_XGB_20) == "score")]
XGB_test_labels <- Test_XGB_20$score
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
knn_predictions <- as.numeric(knn_predictions)
# Calc RMSE & MSE
rmse_knn <- sqrt(mean((knn_predictions - KNN_test_labels)^2))
mse_knn <- mean((knn_predictions - KNN_test_labels)^2)
rmse_knn
mse_knn
#Checking Prediction vs True value of Score when with the model from the 80/20
results_knn <- data.frame(True_Score = KNN_test_labels, Predicted_Score = knn_predictions)
ggplot(results_knn, aes(x = True_Score, y = Predicted_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "KNN: Prediction vs True value of Score",
x = "True Score",
y = "Predicted Score") +
theme_minimal()
set.seed(123)
# Training
LR_test_labels <- test.unseen20$score
# Predicting
unseen.num <- df_as_numeric(test.unseen20)
lr_predictions <- predict(linearReg.model, newdata =unseen.num  )
# Calculate MSE and RMSE
lr_mse <- mean((lr_predictions - unseen.num$score) ^ 2)
lr_rmse <- sqrt(lr_mse)
lr_mse
lr_rmse
#Checking Prediction vs True value of Score when with the model from the 80/20
results_lr <- data.frame(True_Score = LR_test_labels, Predicted_Score = lr_predictions)
ggplot(results_lr, aes(x = True_Score, y = Predicted_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Linear Regression: Prediction vs True value of Score",
x = "True Score",
y = "Predicted Score") +
theme_minimal()
set.seed(123)
# Training
rf_model <- randomForest(x = RF_train_features, y = RF_train_labels, ntree = 100)
# Predicting
rf_predictions <- predict(rf_model, RF_test_features)
# Calculate MSE and RMSE
rf_mse <- mean((rf_predictions - RF_test_labels) ^ 2)
rf_rmse <- sqrt(rf_mse)
rf_mse
rf_rmse
#Checking Prediction vs True value of Score when with the model from the 80/20
results_rf <- data.frame(True_Score = RF_test_labels, Predicted_Score = rf_predictions)
ggplot(results_rf, aes(x = True_Score, y = Predicted_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Random Forest: Prediction vs True value of Score",
x = "True Score",
y = "Predicted Score") +
theme_minimal()
set.seed(123)
# Converting training and testing sets into DMatrix
XGB_dtrain <- xgb.DMatrix(data = as.matrix(XGB_train_features), label = XGB_train_labels)
XGB_dtest <- xgb.DMatrix(data = as.matrix(XGB_test_features), label = XGB_test_labels)
# hyperparameters
params <- list(objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.1,
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8)
# Training
xgb_model <- xgb.train(params = params,data = XGB_dtrain,nrounds = 100, watchlist = list(train = XGB_dtrain, test = XGB_dtest),early_stopping_rounds = 10, print_every_n = 10)
# Evaluating
XGB_prediction <- predict(xgb_model, XGB_dtest)
# Calculate MSE and RMSE
XGB_mse <- mean((XGB_prediction - XGB_test_labels) ^ 2)
XGB_rmse <- sqrt(XGB_mse)
XGB_mse
XGB_rmse
#Checking Prediction vs True value of Score when with the model from the 80/20
results_xgb <- data.frame(True_Score = XGB_test_labels, Predicted_Score = XGB_prediction)
ggplot(results_xgb, aes(x = True_Score, y = Predicted_Score)) +
geom_point(color = "blue", alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "XGBoost: Prediction vs True value of Score",
x = "True Score",
y = "Predicted Score") +
theme_minimal()
mse_knn
rmse_knn
lr_mse
lr_rmse
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
summary(knn_predictions)
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
mean(knn_predictions)
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
summary(knn_predictions)
View(KNN_train_features)
View(KNN_train_features)
View(KNN_train_features)
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_features, k = 5)
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
summary(knn_predictions)
# Training KNN-model with k=5
set.seed(123)
knn_predictions <- knn(train = KNN_train_features, test = KNN_test_features, cl = KNN_train_labels, k = 5)
print(knn_predictions)
View(KNN_test_features)
View(KNN_test_features)
View(KNN_test_features)
View(KNN_test_features)
View(KNN_train_features)
View(KNN_train_features)
